{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Section 1: Dataset Preperation**\n",
        "\n",
        "Dataset pre-processing from the CNN/Daily Mail Kaggle datset. We only used a small subsection due to space and compute constraints of the original dataset with the following ratios:\n",
        "\n",
        "=====\n",
        "\n",
        "Train: 28,000 / 287,113\n",
        "\n",
        "Dev: 1,300 / 13,368\n",
        "\n",
        "Test: 1,100 / 11,490\n",
        "\n",
        "=====\n",
        "\n",
        "We chose to truncate the datasets to around 10% of their original size as training on the original dataset took more than 24 hours on local machines with RTX 4090 GPUs. We did not apply any manual data cleaning, as the dataset was already preprocessed by the original authors. However, we used model-specific tokenization during training and inference, the T5 tokenizer for T5-Small and the GPT-2 tokenizer, for GPT-2.\n",
        "The truncated data is saved as .csv files to the local lab machines on campus and used for the rest of our work.\n"
      ],
      "metadata": {
        "id": "s0T2Odxz50kK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1JGnSxg5wN8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dev = pd.read_csv(\"/home/parker78/NLP_FinalProj/cnn_dailymail/validation.csv\")\n",
        "train = pd.read_csv(\"/home/parker78/NLP_FinalProj/cnn_dailymail/train.csv\")\n",
        "test = pd.read_csv(\"/home/parker78/NLP_FinalProj/cnn_dailymail/test.csv\")\n",
        "\n",
        "train = train.loc[:28000, :]\n",
        "dev = dev.loc[:1300, :]\n",
        "test = test.loc[:1100, :]\n",
        "\n",
        "dev.to_csv(\"cnn_dailymail_dev.csv\")\n",
        "train.to_csv(\"cnn_dailymail_train.csv\")\n",
        "test.to_csv(\"cnn_dailymail_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 2: GPT2 Fine-Tuning**\n",
        "\n",
        "In this section, we did light fine-tuning on GPT2 since it is a purely decoder model and we would be zero-shotting it on summarization without fine-tuning, leading to a much poorer performance.\n",
        "\n",
        "===\n",
        "\n",
        "We use the GPT2 tokenizer and the GPU's on the local lab machine for computation speed. We used a small prompt as GPT2 only takes in a max of 1024 tokens which also includes the prompt. We did implement truncation incase articles went over that threshold as the mean article length was nearly 800 characters.\n",
        "\n",
        "===\n",
        "\n",
        "With space and time constrictions, we did a very small amount of fine-tuning on this model, only running for 1 epoch, with weight decay (reguralization), and learning rate warm-up. This model was saved and was used for our camparative analysis"
      ],
      "metadata": {
        "id": "bGpkMAV86Eqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\") # my sanity check\n",
        "\n",
        "train_dataset = load_dataset('csv', data_files={'train': '/home/parker78/NLP_FinalProj/cnn_dailymail_train.csv'})['train']\n",
        "dev_dataset = load_dataset('csv', data_files={'dev': '/home/parker78/NLP_FinalProj/cnn_dailymail_dev.csv'})['dev']\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    inputs = []\n",
        "    for article, summary in zip(examples[\"article\"], examples[\"highlights\"]):\n",
        "        prompt = \"Briefly summarize this article:\\n\" + article + \"\\nSummary:\"\n",
        "        full_text = prompt + \" \" + summary\n",
        "\n",
        "        tokenized = tokenizer(\n",
        "            full_text,\n",
        "            max_length=1024,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        prompt_ids = tokenizer(prompt, truncation=True, max_length=1024)[\"input_ids\"]\n",
        "        prompt_len = len(prompt_ids)\n",
        "\n",
        "        label_ids = tokenized[\"input_ids\"].copy()\n",
        "        label_ids = [\n",
        "            token if i >= prompt_len and token != tokenizer.pad_token_id else -100\n",
        "            for i, token in enumerate(label_ids)\n",
        "        ]\n",
        "\n",
        "        tokenized[\"labels\"] = label_ids\n",
        "        inputs.append(tokenized)\n",
        "\n",
        "    return {\n",
        "        key: [example[key] for example in inputs]\n",
        "        for key in inputs[0]\n",
        "    }\n",
        "\n",
        "\n",
        "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
        "dev_tokenized = dev_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt_summarizer',\n",
        "    eval_strategy='epoch',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=dev_tokenized\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./gpt_summarizer_model\")\n",
        "tokenizer.save_pretrained(\"./gpt_summarizer_model\")"
      ],
      "metadata": {
        "id": "BLrjiD8c6PCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3: Comparitive Analysis**\n",
        "\n",
        "Here, we get example outputs of the article, highlight (the reference), T5-small's summary, and GPT2's summary. We load up our dataset subsets here while using the lab computers GPU for the models, T5-small and GPT2. Two functions were created, both for their respected models where they would each use the models tokenizer and correct prompting input with the  methods returning the article passed in summarized.\n",
        "\n",
        "===\n",
        "\n",
        "For our metrics, rouge scores were calculated on how close the summarized text was to the highligh (reference). Each summary was appended to an array to easily navigate and ID the correct articles and have correct comparisons between GPT2 and T5-small. We printed out a couple of examples to insepct them for human review and understand the rouge metric better with its comparison. Lastly, the rouge metrics (1,2, L, L-Sum) were all computed and outputted for further analysis between the two models."
      ],
      "metadata": {
        "id": "Lf2ZbOYQ6SIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# T5 (Encoder-Decoder)\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\").to(device)\n",
        "\n",
        "# GPT-2 (Decoder-only)\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"/home/parker78/NLP_FinalProj/gpt_summarizer_model\")\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"/home/parker78/NLP_FinalProj/gpt_summarizer_model\").to(device)\n",
        "\n",
        "# The subset dataset\n",
        "train_dataset = load_dataset('csv', data_files={\"train\": \"/home/parker78/NLP_FinalProj/cnn_dailymail_train.csv\"})[\"train\"]\n",
        "dev_dataset = load_dataset('csv', data_files={\"dev\": \"/home/parker78/NLP_FinalProj/cnn_dailymail_dev.csv\"})[\"dev\"]\n",
        "test_dataset = load_dataset('csv', data_files={\"test\": \"/home/parker78/NLP_FinalProj/cnn_dailymail_test.csv\"})[\"test\"]\n",
        "\n",
        "def summarize_t5(article):\n",
        "    input_text = \"summarize: \" + article\n",
        "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        summary_ids = t5_model.generate(inputs[\"input_ids\"], max_length=150, num_beams=4, early_stopping=True)\n",
        "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def summarize_gpt2(article):\n",
        "    prompt = \"Briefly summarize this article:\\n\" + article + \"\\nSummary:\"\n",
        "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = gpt2_model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=150, num_beams=4, early_stopping=True, pad_token_id=gpt2_tokenizer.eos_token_id)\n",
        "\n",
        "    decoded = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if \"Summary:\" in decoded:\n",
        "        return decoded.split(\"Summary:\")[-1].strip()\n",
        "    else:\n",
        "        return decoded.strip()\n",
        "\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "t5_preds = []\n",
        "gpt2_preds = []\n",
        "refrences = []\n",
        "\n",
        "for example in tqdm(dev_dataset):\n",
        "    article = example[\"article\"]\n",
        "    refrences.append(example[\"highlights\"])\n",
        "\n",
        "    t5_summary = summarize_t5(article)\n",
        "    t5_preds.append(t5_summary)\n",
        "\n",
        "    gpt2_summary = summarize_gpt2(article)\n",
        "    gpt2_preds.append(gpt2_summary)\n",
        "\n",
        "\n",
        "t5_rouge = rouge.compute(predictions=t5_preds, references=refrences, use_stemmer=True)\n",
        "gpt2_rouge = rouge.compute(predictions=gpt2_preds, references=refrences, use_stemmer=True)\n",
        "\n",
        "# printing a couple of examples (did around 3-10 usually)\n",
        "for i in range(10):\n",
        "    print(\"\\nArticle:\", dev_dataset[i][\"article\"][:300], \"... \\n\")\n",
        "    print(\"Reference:\", refrences[i], \"\\n\")\n",
        "    print(\"T5 Summary:\", t5_preds[i], \"\\n\")\n",
        "    print(\"GPT-2 Summary:\", gpt2_preds[i])\n",
        "\n",
        "print(\"T5-small ROUGE:\")\n",
        "print({k: round(v * 100, 2) for k, v in t5_rouge.items()})\n",
        "\n",
        "print(\"\\nGPT-2 ROUGE:\")\n",
        "print({k: round(v * 100, 2) for k, v in gpt2_rouge.items()})"
      ],
      "metadata": {
        "id": "_0lpn5ZO6dkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 4: Other Model Testing**\n",
        "\n",
        "The code block below was also used to try and test on the T5-Base model that allows for up to 768 tokens in it's input sequence. We tried to implement this so that we could attempt to mitigate any issues that may have been caused from an input sequence (the article) from being cut off and disrupting the summary being generated. We set the max output token length to 256 to prevent summaries that were generated from being too long. This also utilized the HuggingFace transformers library to use the T5 model and T5 tokenizer."
      ],
      "metadata": {
        "id": "nr9-YYy45Vug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "#Note: the file links will not work with collab as\n",
        "#these truncated files were run locally on the /tmp drives on the lab machines\n",
        "#as they had storage size.\n",
        "train_data = pd.read_csv(\"/tmp/NLP/CNN_Daily_train.csv\")\n",
        "dev_data = pd.read_csv(\"/tmp/NLP/CNN_Daily_dev.csv\")\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "\n",
        "checkpoint = \"google-t5/t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "# Check for empty, N/A, or null strings\n",
        "print(train_data.isnull().sum())\n",
        "print(dev_data.isnull().sum())\n",
        "\n",
        "#Tokenize the data to be passed into the T5 tokenizer\n",
        "def preprocess_function(examples):\n",
        "    inputs = []\n",
        "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=768, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=256, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# Convert dataframe to huggingface dataset\n",
        "hf_dataset_train = Dataset.from_pandas(train_data)\n",
        "hf_dataset_dev = Dataset.from_pandas(dev_data)\n",
        "\n",
        "tokenized_train = hf_dataset_train.map(preprocess_function, batched=True)\n",
        "tokenized_dev = hf_dataset_dev.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "# Model arguments, change hparams as necessary\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"test_output\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=0.001,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_rougeL\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_dev,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "best_model = AutoModelForSeq2SeqLM.from_pretrained(\"test_output\")\n",
        "summarizer = pipeline(\"summarization\", model=best_model, tokenizer=tokenizer)\n",
        "\n",
        "# Print out articles, summaries, and generated summaries for human evaluation\n",
        "samples = dev_data.sample(5)\n",
        "for i, row in samples.iterrows():\n",
        "    summary = summarizer(prefix + row[\"article\"], max_length=128, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
        "    print(f\"ARTICLE: {row['article'][:400]}...\\n\")\n",
        "    print(f\"MODEL SUMMARY: {summary}\\n\")\n",
        "    print(f\"REFERENCE SUMMARY: {row['highlights']}\\n\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "s0RVI7IQAU7c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}